# -*- coding: utf-8 -*-
"""Transformer-based-Time-series-Prediction-on-Traffic-Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OnRVx5bNXk871seQw0I1pVKWYNNrJtoN
"""

! pip install -q transformers datasets evaluate accelerate "gluonts[torch]" ujson tqdm
! pip install pandas
! pip install sodapy

"""# Data Prepocessing

## Download & cleaning
"""

import pandas as pd
from sodapy import Socrata

# download dataset vis Socrate API for data only from 2021 to 2025
results = Socrata("data.cityofnewyork.us", None).get("h9gi-nx95",
                                                     select="collision_id, crash_date, borough",
                                                     where="crash_date between '2020-12-31T12:00:00' and '2024-12-31T14:00:00'", # 48 months
                                                     limit=500000)

# convert to pandas DataFrame
df = pd.DataFrame.from_records(results)
# check nans, found 'borough' has missing values
df.isna().sum()

# remove rows with nans
df = df[~df['borough'].isna()]
# check number of cases
len(df)

"""## Creat time-series data

"""

# make time series data format
import numpy as np
# Convert crash_date to datetime
df['crash_date'] = pd.to_datetime(df['crash_date'])
df['crash_month'] = df['crash_date'].dt.to_period('M')
df = df.groupby(['borough', 'crash_month']).size().reset_index(name='count')
feat_dynamic_real = np.array([df['crash_month'].dt.year, df['crash_month'].dt.month])# Count accidents per borough per day

# save a df for visualization
df_visual = df.copy()
df.head()

# Pivot the data into a time-series structure
df = df.groupby('borough').agg(
    start=('crash_month', 'min'),
    target=('count', lambda x: list(x))
    ).reset_index()

import numpy as np
# Fit huggingface format
df = df.rename(columns={'borough': 'item_id'})
df["feat_static_cat"] = [[i] for i in range(len(df))]
df['feat_dynamic_real'] = [None for i in range(len(df))]
df.head()

len(df['target'][0]) # 1 row = 48 month of time-series data

"""## EDA"""

import matplotlib.pyplot as plt
import pandas as pd

# Assuming df_visual is already loaded
df_visual['crash_month'] = df_visual['crash_month'].dt.to_timestamp()

# Plot the time-series trends
plt.figure(figsize=(12, 6))
for borough in df_visual["borough"].unique():
    subset = df_visual[df_visual["borough"] == borough]
    plt.plot(subset["crash_month"], subset["count"], marker="o", linestyle="-", label=borough)

plt.xlabel("Date")
plt.ylabel("Number of Accidents")
plt.title("Historical Trend of Traffic Accidents per Borough")
plt.legend()
plt.xticks(rotation=45)
plt.grid(True)
plt.show()

"""## Splitting"""

from datasets import Dataset, DatasetDict
from sklearn.model_selection import train_test_split

prediction_length = 12 # future 12 month

# The validation set contains the same data as the training set, just for a prediction_length longer amount of time.
# This allows us to validate the model's predictions against the ground truth.
train_df = df.copy()
train_df['target'] = df['target'].apply(lambda x: x[:-prediction_length])
train_df['start'] = train_df['start'].dt.to_timestamp()
test_df = df.copy()
test_df['start'] = test_df['start'].dt.to_timestamp()

# Convert pandas DataFrames to Hugging Face Dataset objects
train_dataset = Dataset.from_pandas(train_df,preserve_index=False)
test_dataset = Dataset.from_pandas(test_df,preserve_index=False)
# Create a DatasetDict
dataset_dict = DatasetDict({
    "train": train_dataset,
    "test": test_dataset
})

# Verify the structure
dataset_dict

train_dataset = dataset_dict['train']
test_dataset = dataset_dict['test']

# test has 12 month more than train dataset
len(train_dataset[0]['target']),len(test_dataset[0]['target'])

# make sure all batches has pd.Period data type

from functools import lru_cache

import pandas as pd
import numpy as np

@lru_cache(10_000)
def convert_to_pandas_period(date, freq):
    return pd.Period(date, freq)

def transform_start_field(batch, freq):
    batch["start"] = [convert_to_pandas_period(date, freq) for date in batch["start"]]
    return batch

from functools import partial

freq = "1M"

train_dataset.set_transform(partial(transform_start_field, freq=freq))
test_dataset.set_transform(partial(transform_start_field, freq=freq))

"""## Building data transformation pipeline

### Define transformations
"""

from gluonts.time_feature import get_lags_for_frequency
lags_sequence = get_lags_for_frequency(freq)
print(lags_sequence)

from gluonts.time_feature import time_features_from_frequency_str
time_features = time_features_from_frequency_str(freq)
print(time_features, len(time_features))

from gluonts.time_feature import (
    time_features_from_frequency_str,
    TimeFeature,
    get_lags_for_frequency,
)
from gluonts.dataset.field_names import FieldName
from gluonts.transform import (
    AddAgeFeature,
    AddObservedValuesIndicator,
    AddTimeFeatures,
    AsNumpyArray,
    Chain,
    ExpectedNumInstanceSampler,
    InstanceSplitter,
    RemoveFields,
    SelectFields,
    SetField,
    TestSplitSampler,
    Transformation,
    ValidationSplitSampler,
    VstackFeatures,
    RenameFields,
)

from transformers import PretrainedConfig

def create_transformation(freq: str, config: PretrainedConfig) -> Transformation:
    remove_field_names = []
    if config.num_static_real_features == 0:
        remove_field_names.append(FieldName.FEAT_STATIC_REAL)
    if config.num_dynamic_real_features == 0:
        remove_field_names.append(FieldName.FEAT_DYNAMIC_REAL)
    if config.num_static_categorical_features == 0:
        remove_field_names.append(FieldName.FEAT_STATIC_CAT)

    # a bit like torchvision.transforms.Compose
    return Chain(
        # step 1: remove static/dynamic fields if not specified
        [RemoveFields(field_names=remove_field_names)]
        # step 2: convert the data to NumPy (potentially not needed)
        + (
            [
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_CAT,
                    expected_ndim=1,
                    dtype=int,
                )
            ]
            if config.num_static_categorical_features > 0
            else []
        )
        + (
            [
                AsNumpyArray(
                    field=FieldName.FEAT_STATIC_REAL,
                    expected_ndim=1,
                )
            ]
            if config.num_static_real_features > 0
            else []
        )
        + [
            AsNumpyArray(
                field=FieldName.TARGET,
                # we expect an extra dim for the multivariate case:
                expected_ndim=1 if config.input_size == 1 else 2,
            ),
            # step 3: handle the NaN's by filling in the target with zero
            # and return the mask (which is in the observed values)
            # true for observed values, false for nan's
            # the decoder uses this mask (no loss is incurred for unobserved values)
            # see loss_weights inside the xxxForPrediction model
            AddObservedValuesIndicator(
                target_field=FieldName.TARGET,
                output_field=FieldName.OBSERVED_VALUES,
            ),
            # step 4: add temporal features based on freq of the dataset
            # month of year in the case when freq="M"
            # these serve as positional encodings
            AddTimeFeatures(
                start_field=FieldName.START,
                target_field=FieldName.TARGET,
                output_field=FieldName.FEAT_TIME,
                time_features=time_features_from_frequency_str(freq),
                pred_length=config.prediction_length,
            ),
            # step 5: add another temporal feature (just a single number)
            # tells the model where in its life the value of the time series is,
            # sort of a running counter
            AddAgeFeature(
                target_field=FieldName.TARGET,
                output_field=FieldName.FEAT_AGE,
                pred_length=config.prediction_length,
                log_scale=True,
            ),
            # step 6: vertically stack all the temporal features into the key FEAT_TIME
            VstackFeatures(
                output_field=FieldName.FEAT_TIME,
                input_fields=[FieldName.FEAT_TIME, FieldName.FEAT_AGE]
                + (
                    [FieldName.FEAT_DYNAMIC_REAL]
                    if config.num_dynamic_real_features > 0
                    else []
                ),
            ),
            # step 7: rename to match HuggingFace names
            RenameFields(
                mapping={
                    FieldName.FEAT_STATIC_CAT: "static_categorical_features",
                    FieldName.FEAT_STATIC_REAL: "static_real_features",
                    FieldName.FEAT_TIME: "time_features",
                    FieldName.TARGET: "values",
                    FieldName.OBSERVED_VALUES: "observed_mask",
                }
            ),
        ]
    )

"""### Define InstanceSplitter"""

from gluonts.transform.sampler import InstanceSampler
from typing import Optional

def create_instance_splitter(
    config: PretrainedConfig,
    mode: str,
    train_sampler: Optional[InstanceSampler] = None,
    validation_sampler: Optional[InstanceSampler] = None,
) -> Transformation:
    assert mode in ["train", "validation", "test"]

    instance_sampler = {
        "train": train_sampler
        or ExpectedNumInstanceSampler(
            num_instances=1.0, min_future=config.prediction_length
        ),
        "validation": validation_sampler
        or ValidationSplitSampler(min_future=config.prediction_length),
        "test": TestSplitSampler(),
    }[mode]

    return InstanceSplitter(
        target_field="values",
        is_pad_field=FieldName.IS_PAD,
        start_field=FieldName.START,
        forecast_start_field=FieldName.FORECAST_START,
        instance_sampler=instance_sampler,
        past_length=config.context_length + max(config.lags_sequence),
        future_length=config.prediction_length,
        time_series_fields=["time_features", "observed_mask"],
    )

"""### Create DataLoaders"""

from typing import Iterable

import torch
from gluonts.itertools import Cached, Cyclic
from gluonts.dataset.loader import as_stacked_batches


def create_train_dataloader(
    config: PretrainedConfig,
    freq,
    data,
    batch_size: int,
    num_batches_per_epoch: int,
    shuffle_buffer_length: Optional[int] = None,
    cache_data: bool = True,
    **kwargs,
) -> Iterable:
    PREDICTION_INPUT_NAMES = [
        "past_time_features",
        "past_values",
        "past_observed_mask",
        "future_time_features",
    ]
    if config.num_static_categorical_features > 0:
        PREDICTION_INPUT_NAMES.append("static_categorical_features")

    if config.num_static_real_features > 0:
        PREDICTION_INPUT_NAMES.append("static_real_features")

    TRAINING_INPUT_NAMES = PREDICTION_INPUT_NAMES + [
        "future_values",
        "future_observed_mask",
    ]

    transformation = create_transformation(freq, config)
    transformed_data = transformation.apply(data, is_train=True)
    if cache_data:
        transformed_data = Cached(transformed_data)

    # we initialize a Training instance
    instance_splitter = create_instance_splitter(config, "train")

    # the instance splitter will sample a window of
    # context length + lags + prediction length
    # randomly from within the target time series and return an iterator.
    stream = Cyclic(transformed_data).stream()
    training_instances = instance_splitter.apply(stream)

    return as_stacked_batches(
        training_instances,
        batch_size=batch_size,
        shuffle_buffer_length=shuffle_buffer_length,
        field_names=TRAINING_INPUT_NAMES,
        output_type=torch.tensor,
        num_batches_per_epoch=num_batches_per_epoch,
    )

def create_backtest_dataloader(
    config: PretrainedConfig,
    freq,
    data,
    batch_size: int,
    **kwargs,
):
    PREDICTION_INPUT_NAMES = [
        "past_time_features",
        "past_values",
        "past_observed_mask",
        "future_time_features",
    ]
    if config.num_static_categorical_features > 0:
        PREDICTION_INPUT_NAMES.append("static_categorical_features")

    if config.num_static_real_features > 0:
        PREDICTION_INPUT_NAMES.append("static_real_features")

    transformation = create_transformation(freq, config)
    transformed_data = transformation.apply(data)

    # we create a Validation Instance splitter which will sample the very last
    # context window seen during training only for the encoder.
    instance_sampler = create_instance_splitter(config, "validation")

    # we apply the transformations in train mode
    testing_instances = instance_sampler.apply(transformed_data, is_train=True)

    return as_stacked_batches(
        testing_instances,
        batch_size=batch_size,
        output_type=torch.tensor,
        field_names=PREDICTION_INPUT_NAMES,
    )

def create_test_dataloader(
    config: PretrainedConfig,
    freq,
    data,
    batch_size: int,
    **kwargs,
):
    PREDICTION_INPUT_NAMES = [
        "past_time_features",
        "past_values",
        "past_observed_mask",
        "future_time_features",
    ]
    if config.num_static_categorical_features > 0:
        PREDICTION_INPUT_NAMES.append("static_categorical_features")

    if config.num_static_real_features > 0:
        PREDICTION_INPUT_NAMES.append("static_real_features")

    transformation = create_transformation(freq, config)
    transformed_data = transformation.apply(data, is_train=False)

    # We create a test Instance splitter to sample the very last
    # context window from the dataset provided.
    instance_sampler = create_instance_splitter(config, "test")

    # We apply the transformations in test mode
    testing_instances = instance_sampler.apply(transformed_data, is_train=False)

    return as_stacked_batches(
        testing_instances,
        batch_size=batch_size,
        output_type=torch.tensor,
        field_names=PREDICTION_INPUT_NAMES,
    )

"""## TimeSeriesTransformer Model

### Load model (hugginhface or local)
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')

import os
# %cd /content/drive/MyDrive/my_model/

###
### only excute this when need to use previously trained model saved to local!!!
###
from transformers import TimeSeriesTransformerForPrediction
# Load the model from the local path
model_path = "/content/drive/MyDrive/my_model/trained_models/my_model"
model = TimeSeriesTransformerForPrediction.from_pretrained(model_path)

# alternatively, use this if want to train a new model from huggingface
"""
from transformers import TimeSeriesTransformerConfig, TimeSeriesTransformerForPrediction

config = TimeSeriesTransformerConfig(
    prediction_length = prediction_length,
    context_length= prediction_length*4, # 24 month "look back"
    # lags coming from helper given the freq:
    lags_sequence =  lags_sequence,
    # we'll add 2 time features: monthOfyear and age
    num_time_features = len(time_features_from_frequency_str(freq)) + 1, # 2
    # we have a single static categorical feature, namely time series ID: #
    num_static_categorical_features = 1,
    cardinality=[len(train_dataset)], # # possible values: 5 borough
    # the model will learn an embedding of size 3 for each possible value:
    embedding_dimension=[3],
    # transformer params:
    encoder_layers=4,
    decoder_layers=4,
    d_model=32,
)

model = TimeSeriesTransformerForPrediction(config)

"""

"""### Set model config"""

from transformers import TimeSeriesTransformerConfig

config = TimeSeriesTransformerConfig(
    prediction_length = prediction_length,
    context_length= prediction_length*4, # 24 month "look back"
    # lags coming from helper given the freq:
    lags_sequence =  lags_sequence,
    # we'll add 2 time features: monthOfyear and age
    num_time_features = len(time_features_from_frequency_str(freq)) + 1, # 2
    # we have a single static categorical feature, namely time series ID: #
    num_static_categorical_features = 1,
    cardinality=[len(train_dataset)], # # possible values: 5 borough
    # the model will learn an embedding of size 3 for each possible value:
    embedding_dimension=[3],
    # transformer params:
    encoder_layers=4,
    decoder_layers=4,
    d_model=32,
)

train_dataloader = create_train_dataloader(
    config=config,
    freq=freq,
    data=train_dataset,
    batch_size=256,
    num_batches_per_epoch=100,
)

test_dataloader = create_backtest_dataloader(
    config=config,
    freq=freq,
    data=test_dataset,
    batch_size=64,
)

"""### Check forward pass and initial loss"""

# check dimension
batch = next(iter(train_dataloader))
for k, v in batch.items():
    print(k, v.shape, v.type())

# perform forward pass
outputs = model(
    past_values=batch["past_values"], # [256, 85]
    past_time_features=batch["past_time_features"], # [256, 85, 2]
    past_observed_mask=batch["past_observed_mask"], # [256, 85]
    static_categorical_features=batch["static_categorical_features"] # [256, 1]
    if config.num_static_categorical_features > 0
    else None,
    static_real_features=batch["static_real_features"] # None
    if config.num_static_real_features > 0
    else None,
    future_values=batch["future_values"], # [256, 12]
    future_time_features=batch["future_time_features"], # [256, 12, 2]
    future_observed_mask=batch["future_observed_mask"], # [256, 12]
    output_hidden_states=True,
)

# check initial loss
print("Loss:", outputs.loss.item())

"""### Training loop"""

from accelerate import Accelerator
from torch.optim import AdamW

accelerator = Accelerator()
device = accelerator.device

model.to(device)
optimizer = AdamW(model.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)

model, optimizer, train_dataloader = accelerator.prepare(
    model,
    optimizer,
    train_dataloader,
)

model.train()
loss_record_1 = []
for epoch in range(200):
    for idx, batch in enumerate(train_dataloader):
        optimizer.zero_grad()
        outputs = model(
            static_categorical_features=batch["static_categorical_features"].to(device)
            if config.num_static_categorical_features > 0
            else None,
            static_real_features=batch["static_real_features"].to(device)
            if config.num_static_real_features > 0
            else None,
            past_time_features=batch["past_time_features"].to(device),
            past_values=batch["past_values"].to(device),
            future_time_features=batch["future_time_features"].to(device),
            future_values=batch["future_values"].to(device),
            past_observed_mask=batch["past_observed_mask"].to(device),
            future_observed_mask=batch["future_observed_mask"].to(device),
        )
        loss = outputs.loss

        # Backpropagation
        accelerator.backward(loss)
        optimizer.step()

        if idx % 100 == 0:
            loss_record_1.append(loss.item())
            print(loss.item())

# Save the trained model to a local folder.
import os
model_path = "/content/drive/MyDrive/my_model/trained_models/my_model"
model.save_pretrained(model_path)

print(f"Model saved to: {model_path}")

"""### Testing

#### Loss curve
"""

# Plot the loss curves
import matplotlib.pyplot as plt

epochs = list(range(1, len(loss_record_1) + 1))
plt.figure(figsize=(10, 5))
plt.plot(epochs, loss_record_1, marker='o', linestyle='-', color='b', markersize=3, label="Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("TimeSeriesTransformer: Training Loss Curves")
plt.legend()
plt.grid(True)
plt.show()

"""#### Prediction"""

from accelerate import Accelerator
from torch.optim import AdamW

accelerator = Accelerator()
device = accelerator.device

model.eval()

forecasts = []

for batch in test_dataloader:
    outputs = model.generate(
        static_categorical_features=batch["static_categorical_features"].to(device)
        if config.num_static_categorical_features > 0
        else None,
        static_real_features=batch["static_real_features"].to(device)
        if config.num_static_real_features > 0
        else None,
        past_time_features=batch["past_time_features"].to(device),
        past_values=batch["past_values"].to(device),
        future_time_features=batch["future_time_features"].to(device),
        past_observed_mask=batch["past_observed_mask"].to(device),
    )
    forecasts.append(outputs.sequences.cpu().numpy())

forecasts = np.vstack(forecasts)
print("forecasts.shape", forecasts.shape)

from evaluate import load
from gluonts.time_feature import get_seasonality

mase_metric = load("evaluate-metric/mase")
smape_metric = load("evaluate-metric/smape")
forecast_median = np.median(forecasts, 1)

mase_metrics = []
smape_metrics = []
for item_id, ts in enumerate(test_dataset):
    training_data = ts["target"][:-prediction_length]
    ground_truth = ts["target"][-prediction_length:]
    mase = mase_metric.compute(
        predictions=forecast_median[item_id],
        references=np.array(ground_truth),
        training=np.array(training_data),
        periodicity=get_seasonality(freq))
    mase_metrics.append(mase["mase"])

    smape = smape_metric.compute(
        predictions=forecast_median[item_id],
        references=np.array(ground_truth),
    )
    smape_metrics.append(smape["smape"])

print(f"\nMASE: {np.mean(mase_metrics)}")
print(f"\nsMAPE: {np.mean(smape_metrics)}")

"""#### Visualization"""

import matplotlib.pyplot as plt

plt.scatter(mase_metrics, smape_metrics, alpha=0.3)
plt.xlabel("MASE")
plt.ylabel("sMAPE")
plt.show()

import matplotlib.dates as mdates
import matplotlib.pyplot as plt

def plot_on_axis(ax, ts_index):
    # Generate the datetime index from the test dataset
    index = pd.period_range(
        start=test_dataset[ts_index][FieldName.START],
        periods=len(test_dataset[ts_index][FieldName.TARGET]),
        freq=freq,
    ).to_timestamp()

    # Set major ticks every January (or adjust as needed) and minor ticks every month
    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 13)))
    ax.xaxis.set_minor_locator(mdates.MonthLocator())

    # Plot the actual values
    ax.plot(
        index[1 * prediction_length:],
        test_dataset[ts_index]["target"][1 * prediction_length:],
        label="actual",
    )

    # Plot the median forecast
    ax.plot(
        index[-prediction_length:],
        np.median(forecasts[ts_index], axis=0),
        label="median",
    )

    # Plot the confidence interval (mean +/- std)
    ax.fill_between(
        index[-prediction_length:],
        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0),
        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0),
        alpha=0.3,
        interpolate=True,
        label="+/- 1-std",
    )

    borough_names = ["BRONX", "BROOKLYN", "MANHATTAN", "QUEENS", "STATEN ISLAND"]
    ax.legend()
    ax.set_title(f"{borough_names[ts_index]} borough")
    # Set x and y axis labels for the subplot
    ax.set_xlabel("Number of Car Crash")
    ax.set_ylabel("Time")

# Create a figure with 4 subplots stacked vertically
fig, axs = plt.subplots(5, 1, figsize=(10, 12), sharex=True)

# Loop over the axes and corresponding ts_index values (assuming ts_index 0 to 3)
for ts_index, ax in enumerate(axs):
    plot_on_axis(ax, ts_index)

fig.suptitle("TimeSeriesTransformer Prediction on Car Crash in NYC \n", fontsize=16)
plt.tight_layout()
plt.show()

"""## Autoformer Model

### Load model and set model config
"""

from transformers import AutoformerConfig, AutoformerForPrediction

config_2 = AutoformerConfig(
    prediction_length = prediction_length,
    context_length= prediction_length*4, # 24 month "look back"
    # lags coming from helper given the freq:
    lags_sequence =  lags_sequence,
    # we'll add 2 time features: monthOfyear and age
    num_time_features = len(time_features_from_frequency_str(freq)) + 1, # 2
    # we have a single static categorical feature, namely time series ID: #
    num_static_categorical_features = 1,
    cardinality=[len(train_dataset)], # # possible values: 5 borough
    # the model will learn an embedding of size 3 for each possible value:
    embedding_dimension=[3],
    # transformer params:
    encoder_layers=4,
    decoder_layers=4,
    d_model=32,
)

model_2 = AutoformerForPrediction(config_2)

train_dataloader = create_train_dataloader(
    config=config_2,
    freq=freq,
    data=train_dataset,
    batch_size=256,
    num_batches_per_epoch=100,
)

test_dataloader = create_backtest_dataloader(
    config=config_2,
    freq=freq,
    data=test_dataset,
    batch_size=64,
)

"""### Check forward pass and initial loss"""

# check dimension
batch = next(iter(train_dataloader))
for k, v in batch.items():
    print(k, v.shape, v.type())


# perform forward pass
outputs = model_2(
    past_values=batch["past_values"], # [256, 85]
    past_time_features=batch["past_time_features"], # [256, 85, 2]
    past_observed_mask=batch["past_observed_mask"], # [256, 85]
    static_categorical_features=batch["static_categorical_features"] # [256, 1]
    if config_2.num_static_categorical_features > 0
    else None,
    static_real_features=batch["static_real_features"] # None
    if config_2.num_static_real_features > 0
    else None,
    future_values=batch["future_values"], # [256, 12]
    future_time_features=batch["future_time_features"], # [256, 12, 2]
    future_observed_mask=batch["future_observed_mask"], # [256, 12]
    output_hidden_states=True,
)

# check initial loss
print("Loss:", outputs.loss.item())

"""### Training loop"""

from accelerate import Accelerator
from torch.optim import AdamW

accelerator = Accelerator()
device = accelerator.device

model_2.to(device)
optimizer = AdamW(model_2.parameters(), lr=6e-4, betas=(0.9, 0.95), weight_decay=1e-1)

model_2, optimizer, train_dataloader = accelerator.prepare(
    model_2,
    optimizer,
    train_dataloader,
)

model_2.train()
loss_record_2 = []
for epoch in range(200):
    for idx, batch in enumerate(train_dataloader):
        optimizer.zero_grad()
        outputs = model_2(
            static_categorical_features=batch["static_categorical_features"].to(device)
            if config_2.num_static_categorical_features > 0
            else None,
            static_real_features=batch["static_real_features"].to(device)
            if config_2.num_static_real_features > 0
            else None,
            past_time_features=batch["past_time_features"].to(device),
            past_values=batch["past_values"].to(device),
            future_time_features=batch["future_time_features"].to(device),
            future_values=batch["future_values"].to(device),
            past_observed_mask=batch["past_observed_mask"].to(device),
            future_observed_mask=batch["future_observed_mask"].to(device),
        )
        loss = outputs.loss

        # Backpropagation
        accelerator.backward(loss)
        optimizer.step()

        if idx % 100 == 0:
            loss_record_2.append(loss.item())
            print(loss.item())

# Save the trained model to a local folder.
import os
model_path = "/content/drive/MyDrive/my_model/trained_models/my_model_2"
model_2.save_pretrained(model_path)

print(f"Model saved to: {model_path}")



"""### Prediction"""

from accelerate import Accelerator
from torch.optim import AdamW

accelerator = Accelerator()
device = accelerator.device

model_2.eval()

forecasts = []

for batch in test_dataloader:
    outputs = model_2.generate(
        static_categorical_features=batch["static_categorical_features"].to(device)
        if config_2.num_static_categorical_features > 0
        else None,
        static_real_features=batch["static_real_features"].to(device)
        if config_2.num_static_real_features > 0
        else None,
        past_time_features=batch["past_time_features"].to(device),
        past_values=batch["past_values"].to(device),
        future_time_features=batch["future_time_features"].to(device),
        past_observed_mask=batch["past_observed_mask"].to(device),
    )
    forecasts.append(outputs.sequences.cpu().numpy())

forecasts = np.vstack(forecasts)
print("forecasts.shape", forecasts.shape)

from evaluate import load
from gluonts.time_feature import get_seasonality

mase_metric = load("evaluate-metric/mase")
smape_metric = load("evaluate-metric/smape")
forecast_median = np.median(forecasts, 1)

mase_metrics = []
smape_metrics = []
for item_id, ts in enumerate(test_dataset):
    training_data = ts["target"][:-prediction_length]
    ground_truth = ts["target"][-prediction_length:]
    mase = mase_metric.compute(
        predictions=forecast_median[item_id],
        references=np.array(ground_truth),
        training=np.array(training_data),
        periodicity=get_seasonality(freq))
    mase_metrics.append(mase["mase"])

    smape = smape_metric.compute(
        predictions=forecast_median[item_id],
        references=np.array(ground_truth),
    )
    smape_metrics.append(smape["smape"])

print(f"\nMASE: {np.mean(mase_metrics)}")
print(f"\nsMAPE: {np.mean(smape_metrics)}")

"""### Visualization"""

import matplotlib.pyplot as plt

plt.scatter(mase_metrics, smape_metrics, alpha=0.3)
plt.xlabel("MASE")
plt.ylabel("sMAPE")
plt.show()

# Plot the loss curves
import matplotlib.pyplot as plt

epochs = list(range(1, len(loss_record_2) + 1))
plt.figure(figsize=(10, 5))
plt.plot(epochs, loss_record_2, marker='o', linestyle='-', color='b', markersize=3, label="Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("Autoformer: Training Loss Curves")
plt.legend()
plt.grid(True)
plt.show()

import matplotlib.dates as mdates
import matplotlib.pyplot as plt

def plot_on_axis(ax, ts_index):
    # Generate the datetime index from the test dataset
    index = pd.period_range(
        start=test_dataset[ts_index][FieldName.START],
        periods=len(test_dataset[ts_index][FieldName.TARGET]),
        freq=freq,
    ).to_timestamp()

    # Set major ticks every January (or adjust as needed) and minor ticks every month
    ax.xaxis.set_major_locator(mdates.MonthLocator(bymonth=(1, 13)))
    ax.xaxis.set_minor_locator(mdates.MonthLocator())

    # Plot the actual values
    ax.plot(
        index[1 * prediction_length:],
        test_dataset[ts_index]["target"][1 * prediction_length:],
        label="actual",
    )

    # Plot the median forecast
    ax.plot(
        index[-prediction_length:],
        np.median(forecasts[ts_index], axis=0),
        label="median",
    )

    # Plot the confidence interval (mean +/- std)
    ax.fill_between(
        index[-prediction_length:],
        forecasts[ts_index].mean(0) - forecasts[ts_index].std(axis=0),
        forecasts[ts_index].mean(0) + forecasts[ts_index].std(axis=0),
        alpha=0.3,
        interpolate=True,
        label="+/- 1-std",
    )

    borough_names = ["BRONX", "BROOKLYN", "MANHATTAN", "QUEENS", "STATEN ISLAND"]
    ax.legend()
    ax.set_title(f"{borough_names[ts_index]} borough")
    # Set x and y axis labels for the subplot
    ax.set_xlabel("Number of Car Crash")
    ax.set_ylabel("Time")

# Create a figure with 5 subplots stacked vertically
fig, axs = plt.subplots(5, 1, figsize=(10, 12), sharex=True)

# Loop over the axes and corresponding ts_index values (assuming ts_index 0 to 5)
for ts_index, ax in enumerate(axs):
    plot_on_axis(ax, ts_index)

fig.suptitle("Autoformer Prediction on Car Crash in NYC \n", fontsize=16)
plt.tight_layout()
plt.show()
