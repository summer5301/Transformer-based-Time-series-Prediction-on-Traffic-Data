@misc{siaminamini2018forecastingeconomicsfinancialtime,
      title={Forecasting Economics and Financial Time Series: ARIMA vs. LSTM}, 
      author={Sima Siami-Namini and Akbar Siami Namin},
      year={2018},
      eprint={1803.06386},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1803.06386}, 
}

@article{Taylor02012018,
author = {Sean J. Taylor and Benjamin Letham and},
title = {Forecasting at Scale},
journal = {The American Statistician},
volume = {72},
number = {1},
pages = {37--45},
year = {2018},
publisher = {ASA Website},
doi = {10.1080/00031305.2017.1380080},
URL = {   
        https://doi.org/10.1080/00031305.2017.1380080
},
eprint = {   
        https://doi.org/10.1080/00031305.2017.1380080
}
}

@article{5ef6cb54-fcbb-3296-a35d-5d900acfd06b,
 ISSN = {00359254, 14679876},
 URL = {http://www.jstor.org/stable/2347162},
 abstract = {The Holt-Winters forecasting procedure is a simple widely used projection method which can cope with trend and seasonal variation. However, empirical studies have tended to show that the method is not as accurate on average as the more complicated Box-Jenkins procedure. This paper points out that these empirical studies have used the automatic version of the method, whereas a non-automatic version is also possible in which subjective judgement is employed, for example, to choose the correct model for seasonality. The paper re-analyses seven series from the Newbold-Granger study for which Box-Jenkins forecasts were reported to be much superior to the (automatic) Holt-Winters forecasts. The series do not appear to have any common properties, but it is shown that the automatic Holt-Winters forecasts can often be improved by subjective modifications. It is argued that a fairer comparison would be that between Box-Jenkins and a non-automatic version of Holt-Winters. Some general recommendations are made concerning the choice of a univariate forecasting procedure. The paper also makes suggestions regarding the implementation of the Holt-Winters procedure, including a choice of starting values.},
 author = {C. Chatfield},
 journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
 number = {3},
 pages = {264--279},
 publisher = {[Royal Statistical Society, Oxford University Press]},
 title = {The Holt-Winters Forecasting Procedure},
 urldate = {2025-03-21},
 volume = {27},
 year = {1978}
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@misc{loshchilov2019decoupledweightdecayregularization,
      title={Decoupled Weight Decay Regularization}, 
      author={Ilya Loshchilov and Frank Hutter},
      year={2019},
      eprint={1711.05101},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1711.05101}, 
}

@misc{wen2023transformerstimeseriessurvey,
      title={Transformers in Time Series: A Survey}, 
      author={Qingsong Wen and Tian Zhou and Chaoli Zhang and Weiqi Chen and Ziqing Ma and Junchi Yan and Liang Sun},
      year={2023},
      eprint={2202.07125},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2202.07125}, 
}

@misc{wu2022autoformerdecompositiontransformersautocorrelation,
      title={Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting}, 
      author={Haixu Wu and Jiehui Xu and Jianmin Wang and Mingsheng Long},
      year={2022},
      eprint={2106.13008},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2106.13008}, 
}

@misc{rasul2024lagllamafoundationmodelsprobabilistic,
      title={Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting}, 
      author={Kashif Rasul and Arjun Ashok and Andrew Robert Williams and Hena Ghonia and Rishika Bhagwatkar and Arian Khorasani and Mohammad Javad Darvishi Bayazi and George Adamopoulos and Roland Riachi and Nadhir Hassen and Marin Bilo≈° and Sahil Garg and Anderson Schneider and Nicolas Chapados and Alexandre Drouin and Valentina Zantedeschi and Yuriy Nevmyvaka and Irina Rish},
      year={2024},
      eprint={2310.08278},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.08278}, 
}

@misc{hu2024timessmsimplifyingunifyingstate,
      title={Time-SSM: Simplifying and Unifying State Space Models for Time Series Forecasting}, 
      author={Jiaxi Hu and Disen Lan and Ziyu Zhou and Qingsong Wen and Yuxuan Liang},
      year={2024},
      eprint={2405.16312},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16312}, 
}

@ARTICLE{1257413,
  author={Cao, L.J. and Tay, F.E.H.},
  journal={IEEE Transactions on Neural Networks}, 
  title={Support vector machine with adaptive parameters in financial time series forecasting}, 
  year={2003},
  volume={14},
  number={6},
  pages={1506-1518},
  keywords={Support vector machines;Neural networks;Machine learning;Multi-layer neural network;Pattern recognition;Risk management;Contracts;Predictive models;Economic forecasting;Mechanical engineering},
  doi={10.1109/TNN.2003.820556}}


@Article{a10040114,
AUTHOR = {Tyralis, Hristos and Papacharalampous, Georgia},
TITLE = {Variable Selection in Time Series Forecasting Using Random Forests},
JOURNAL = {Algorithms},
VOLUME = {10},
YEAR = {2017},
NUMBER = {4},
ARTICLE-NUMBER = {114},
URL = {https://www.mdpi.com/1999-4893/10/4/114},
ISSN = {1999-4893},
ABSTRACT = {Time series forecasting using machine learning algorithms has gained popularity recently. Random forest is a machine learning algorithm implemented in time series forecasting; however, most of its forecasting properties have remained unexplored. Here we focus on assessing the performance of random forests in one-step forecasting using two large datasets of short time series with the aim to suggest an optimal set of predictor variables. Furthermore, we compare its performance to benchmarking methods. The first dataset is composed by 16,000 simulated time series from a variety of Autoregressive Fractionally Integrated Moving Average (ARFIMA) models. The second dataset consists of 135 mean annual temperature time series. The highest predictive performance of RF is observed when using a low number of recent lagged predictor variables. This outcome could be useful in relevant future applications, with the prospect to achieve higher predictive accuracy.},
DOI = {10.3390/a10040114}
}

@misc{kong2025unlockingpowerlstmlong,
      title={Unlocking the Power of LSTM for Long Term Time Series Forecasting}, 
      author={Yaxuan Kong and Zepu Wang and Yuqi Nie and Tian Zhou and Stefan Zohren and Yuxuan Liang and Peng Sun and Qingsong Wen},
      year={2025},
      eprint={2408.10006},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.10006}, 
}

@misc{zeng2023financialtimeseriesforecasting,
      title={Financial Time Series Forecasting using CNN and Transformer}, 
      author={Zhen Zeng and Rachneet Kaur and Suchetha Siddagangappa and Saba Rahimi and Tucker Balch and Manuela Veloso},
      year={2023},
      eprint={2304.04912},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2304.04912}, 
}

@article{Han1999EfficientMO,
  title={Efficient mining of partial periodic patterns in time series database},
  author={Jiawei Han and Guozhu Dong and Yiwen Yin},
  journal={Proceedings 15th International Conference on Data Engineering (Cat. No.99CB36337)},
  year={1999},
  pages={106-115},
  url={https://api.semanticscholar.org/CorpusID:6388610}
}
